# encoding: utf-8
# module torch._C._return_types
# from C:\Users\hp\PycharmProjects\Text_Summarizer\venv\lib\site-packages\functorch\_C.cp38-win_amd64.pyd
# by generator 1.147
# no doc
# no imports

from .tuple import tuple

class _scaled_dot_product_flash_attention(tuple):
    # no doc
    def __init__(self, *args, **kwargs): # real signature unknown
        pass

    @staticmethod # known case of __new__
    def __new__(*args, **kwargs): # real signature unknown
        """ Create and return a new object.  See help(type) for accurate signature. """
        pass

    def __reduce__(self, *args, **kwargs): # real signature unknown
        pass

    def __repr__(self, *args, **kwargs): # real signature unknown
        """ Return repr(self). """
        pass

    cum_seq_k = property(lambda self: object(), lambda self, v: None, lambda self: None)  # default

    cum_seq_q = property(lambda self: object(), lambda self, v: None, lambda self: None)  # default

    debug_attn_mask = property(lambda self: object(), lambda self, v: None, lambda self: None)  # default

    logsumexp = property(lambda self: object(), lambda self, v: None, lambda self: None)  # default

    max_k = property(lambda self: object(), lambda self, v: None, lambda self: None)  # default

    max_q = property(lambda self: object(), lambda self, v: None, lambda self: None)  # default

    ouput = property(lambda self: object(), lambda self, v: None, lambda self: None)  # default

    philox_offset = property(lambda self: object(), lambda self, v: None, lambda self: None)  # default

    philox_seed = property(lambda self: object(), lambda self, v: None, lambda self: None)  # default


    n_fields = 9
    n_sequence_fields = 9
    n_unnamed_fields = 0


